---
title: 'arXiv Person Re-identification'
date: 2017-11-12
permalink: /posts/2017/11/arxiv/
tags:
  - arXiv
  - Person Re-identification
  - Computer Vision
---

arXiv与Person Re-identification相关。

<span id="back"></span>
## List

1. [In Defense of the Triplet Loss for Person Re-Identification](#jump1) [[pdf](https://arxiv.org/pdf/1703.07737)] [[github](https://github.com/VisualComputingInstitute/triplet-reid)]
2. SVDNet for Pedestrian Retrieval [[pdf](https://arxiv.org/pdf/1703.05693)] [[github](https://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval)]

## Notes

<span id="jump1"></span>
## 1. In Defense of the Triplet Loss for Person Re-Identification [Back](#back)

又一篇基于 Triplet 的力作。在 re-id 社区认为一个 classification loss 联合一个 verification loss 对这个问题更有效时，再一次用一种改进的 Triplet loss 实现了端到端的特征提取加度量学习的深度学习方法，超过了目前 state-of-the-art 的方法一个较大的边界。文章中作者首先指出了 Classification 和 Verification 两种 loss 方法的明显缺点：

- Classification：随着行人 id 的增长，网络中需要学习的参数也将迅速增长，然而这些增长在训练结束后并没有多大的意义。
- Verification：这种 loss 需用于图片之间，如比较两张图片的相似度，当面临行人检索等问题时，需要一对一对的进行比较，这将大大增加测试时所投入的代价。

而使用 Triplet loss 能够明显避免以上缺点，进行 End-to-end 的特征提取与度量学习的训练，并最终通过简单的距离（如欧式距离）计算，在特征空间中对行人间的距离进行比较。

然而 Triplet 使用时的难点在于：

- 如果使用朴素的 Triplet，并不能得到很好地结果。Triplet 中一个很重要部分是如何挖掘 Hard Triplets 进行有效的特征学习，否则训练将会很快停滞不前。
- 如果一味追求 Hard Triplets，将会非常耗时，且无法明确定义什么是一个“好”的 Hard Triplets。此外，选用过多的 Hard Triplets 还可能会使训练变得十分不稳定而无法收敛。

总的来说本文有两点贡献：
- 首先，本文提出了一种含有一定 Hard Triplets 挖掘能力的经典 Triplet loss 的变体，并系统地评测了各种 Triplet loss 变体的性能。
- 与以往的认识不同，本文还展示了仅仅使用 Triplet loss 而无需添加特殊的网络层级结构等其它方法，在预训练好的 CNN 和从头开始训练的网络模型上，都能取得 state-of-the-art 的结果。

### Triplet Loss & Importance of Mining

FaceNet针对LMNN loss提取了其改进版本，命名为Triplet loss：

<div align="center">
<img src="/images/arxiv2017/eq-1.PNG" />
</div>

![Triplet loss](/images/arxiv2017/eq-2.PNG)

如果在整个数据集上经过足够长时间的训练，Triplet最终能将所有的正样本pull到一起。然而当数据集变得越来越大时，triplets的数量将呈现三次指数增长，使得进行充分训练变得不切实际。此外，通过triplet loss学习到的映射将很快就能正确判断大部分普通的triplets，导致大量triplets在训练时将毫无作用。因此，进行Hard Triplets的挖掘变得十分重要。Hard Triplets包含hard negatives（相似外观的不同行人）和hard positives（外观姿态变化较大的同一行人）。




[Back](#back)
