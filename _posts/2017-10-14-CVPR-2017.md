---
title: 'CVPR 2017 Person Re-identification'
date: 2017-10-14
permalink: /posts/2017/10/cvpr-2017/
tags:
  - CVPR
  - Person Re-identification
  - Computer Vision
---

CVPR 2017 Person Re-ID 相关论文。

<span id="back"></span>
# List

1. [Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification](#jump1) [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Learning_Deep_Context-Aware_CVPR_2017_paper.pdf)] [[supp](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Learning_Deep_Context-Aware_2017_CVPR_supplemental.pdf)]
2. [Beyond Triplet Loss: A Deep Quadruplet Network for Person Re-Identification](#jump2) [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Beyond_Triplet_Loss_CVPR_2017_paper.pdf)] [[github](https://github.com/yokattame/SpindleNet)]
3. Spindle Net: Person Re-Identification With Human Body Region Guided Feature Decomposition and Fusion [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf)] [[github](https://github.com/yokattame/SpindleNet)]
4. Re-Ranking Person Re-Identification With k-Reciprocal Encoding [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf)]
5. Sequential Person Recognition in Photo Albums With a Recurrent Network [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Sequential_Person_Recognition_CVPR_2017_paper.pdf)]
6. Person Re-Identification in the Wild
[[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zheng_Person_Re-Identification_in_CVPR_2017_paper.pdf)] [[github](https://github.com/liangzheng06/PRW-baseline)]
7. Person Search With Natural Language Description [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Person_Search_With_CVPR_2017_paper.pdf)]
8. PoseTrack: Joint Multi-Person Pose Estimation and Tracking [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Iqbal_PoseTrack_Joint_Multi-Person_CVPR_2017_paper.pdf)]
9. Scalable Person Re-Identification on Supervised Smoothed Manifold [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Bai_Scalable_Person_Re-Identification_CVPR_2017_paper.pdf)] [[supp](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Bai_Scalable_Person_Re-Identification_2017_CVPR_supplemental.pdf)]
10. One-Shot Metric Learning for Person Re-Identification [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Bak_One-Shot_Metric_Learning_CVPR_2017_paper.pdf)]
11. Joint Detection and Identification Feature Learning for Person Search [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xiao_Joint_Detection_and_CVPR_2017_paper.pdf)] [[github](https://github.com/ShuangLI59/person_search)]
12. Multiple People Tracking by Lifted Multicut and Person Re-Identification [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Tang_Multiple_People_Tracking_CVPR_2017_paper.pdf)]
13. [Point to Set Similarity Based Deep Feature Learning for Person Re-Identification](#jump13) [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Point_to_Set_CVPR_2017_paper.pdf)]
14. Fast Person Re-Identification via Cross-Camera Semantic Binary Transformation [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Fast_Person_Re-Identification_CVPR_2017_paper.pdf)]
15. See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-Identification [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_See_the_Forest_CVPR_2017_paper.pdf)]
16. Identifying First-Person Camera Wearers in Third-Person Videos [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Fan_Identifying_First-Person_Camera_CVPR_2017_paper.pdf)]
17. Fully-Adaptive Feature Sharing in Multi-Task Networks With Applications in Person Attribute Classification [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Fully-Adaptive_Feature_Sharing_CVPR_2017_paper.pdf)]
18. [Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network](#jump18) [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Consistent-Aware_Deep_Learning_CVPR_2017_paper.pdf)]
19. Pose-Aware Person Recognition [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Kumar_Pose-Aware_Person_Recognition_CVPR_2017_paper.pdf)] [[supp](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kumar_Pose-Aware_Person_Recognition_2017_CVPR_supplemental.pdf)]
20. Unsupervised Adaptive Re-Identification in Open World Dynamic Camera Networks [[pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Panda_Unsupervised_Adaptive_Re-Identification_CVPR_2017_paper.pdf)] [[supp](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Panda_Unsupervised_Adaptive_Re-Identification_2017_CVPR_supplemental.pdf)]
21. Quality Aware Network for Set to Set Recognition [[pdf](https://arxiv.org/pdf/1704.03373.pdf)] [[github](https://github.com/sciencefans/Quality-Aware-Network)]

# Notes

<span id="jump1"></span>
## 1. Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification [Back](#back)

**Outline**
- 使用多尺度的空间上下文感知网络结构MSCAN去更好的捕捉小目标线索（eg.鞋、包）
- 使用空间转换网络STN去定位行人身体各个部分，学习更好的基于行人global和local的特征

**Motivation**

作者认为传统的DCNN模型在ReID的背景下存在以下两个问题：
- 现在使用的DCNN模型通常会堆叠很多层的conv+pool获得深度网络，然后随着层数的增加，网络会很容易忽视掉ReID中很重要的小尺度视觉线索，比如太阳镜和鞋子。
- 由于现有的数据集中存在的行人姿态变化和不准确的行人检测，大量行人存在未对齐或部件缺失的情况。这时对于一些基于局部的表示，如果用预定义好的网格将无法正确的捕捉对应的局部特征。

为了解决第一个问题，作者使用了MSCAN(Multi-Scale Context-Aware Network)结构，对于每一个conv层，应用了多个具有不同感受野的卷积核获得多个feature map，之后进行concat。在具体实践时，使用了膨胀卷积方法（dilated conv），这样既能获得不同的感受野，而又不增加信息冗余：

<div align="center" >
<img src="/images/cvpr2017/mscan-1.PNG" width="60%" align="center" />
</div>

在学习局部特征表示时，与之前使用固定的网格分割不同，针对行人未对齐的问题，作者使用了STN(Spatial Transform Networks)去学习和定位行人的局部部件。STN中的空间定位网络学习变换参数，之后网格生成器生成grid用于对输入图像的局部采样。此处用到了4个变换参数，s为尺度变换参数，t为平移变换参数。

<div align="center" >
<img src="/images/cvpr2017/mscan-2.PNG" width="40%" align="center" />
</div>

整个网络的框架如下：

<div align="center" >
<img src="/images/cvpr2017/mscan-3.PNG" width="100%" align="center" />
</div>

**Results**

|metric|Marke-t1501(SQ)|Market-1501(MQ)|CUHK03(manually)|CUHK03(detected)|MARS(SQ)|MARS(MQ)|
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
|**CMC top-1**|**80.31**|**86.79**|**67.99**|**74.21**|**71.77**|**83.03**|
|**mAP**|**57.53**|**66.70**|-|-|-|-|

<span id="jump2"></span>
## 2. Beyond Triplet Loss: A Deep Quadruplet Network for Person Re-Identification [Back](#back)

**Outline**
- 四路网络
- 基于阈值的Hard Negative Mining

**Motivation**

Triplet loss训练出来的模型往往有着较大的类内误差，泛化能力不强。而本文使用Quadruplet loss，虽不能明显提高其在训练集上的表现，但却能减小类内误差，提高其泛化能力，使其在测试集上的表现得到提高。

<div align="center" >
<img src="/images/cvpr2017/qua-1.PNG" width="60%" align="center" />
</div>

Quadruplet loss由两部分组成，在传统的Triplet loss的基础上，加入了与probe无关的负样本对的约束。这种约束一定程度上要求最小类间距离需大于最大类内距离，能进一步提高其泛化能力。其对应的网络结构如下，其中阴影部分为新增约束。

<div align="center" >
<img src="/images/cvpr2017/qua-2.PNG" width="100%" align="center" />
</div>

除此之外，文章还提出了“基于阈值的Online Hard Negative Mining”，Triplet loss 的训练进入中后期时，Hard Triplets对于网络的进一步提高有着至关重要的作用。本文通过自适应的方式进行在线Hard Negative Mining，用于进一步提高网络的性能。

**Results**

|metric|CHK03(manually)|CUHK01(p=486)|CUHK01(p=100)|VIPeR|
|:-----:|:-----:|:-----:|:-----:|:-----:|
|**CMC top-1**|**75.53**|**62.55**|**81.00**|**49.05**|

<span id="jump13"></span>
## 13. Point to Set Similarity Based Deep Feature Learning for Person Re-Identification [Back](#back)

**Outline**

- P2S的模型，以及基于P2S的改进的pairwise loss和triplet loss
- global+local的CNN特征表示与学习方法

**Motivation**

传统的Person Re-ID在距离比较上不论时pairwise还是triplet用的都是P2P(Point to Point)的，本文提出了使用P2S(Point to Set)的距离度量用于联合优化类内距离最小化和类间距离最大化问题。与基于P2S的距离度量方法相比，P2S的方法能更有效地提高Re-ID在Ranking上地表现。

P2S的度量由三部分组成，分别为pairwise项、triplet项和正则化项：

<div align="center" >
<img src="/images/cvpr2017/p2s-1.PNG" width="60%" align="center" />
</div>

pairwise项通过随机选取正负样本进行训练而减轻过拟合问题，triplet项自适应地选取小于一定边界的样本用于提升Ranking的表现，而正则化项则用于平滑参数维持模型的数值稳定。其中P2S的pairwise项如下：

<div align="center" >
<img src="/images/cvpr2017/p2s-2.PNG" width="60%" align="center" />
</div>

i、j为行人的ID，r为训练集中该ID的所有训练样本，而<img src="http://chart.googleapis.com/chart?cht=tx&chl=$G^a_{i,j}(j,r)=1$" style="border:none;">意味着第j个ID的第r张图片与ID为i的anchor img是同一个人。

<div align="center" >
<img src="/images/cvpr2017/p2s-3.PNG" width="40%" align="center" />
</div>

Triplet项用了一种改进形式的对称式的Triplet，在类间距离部分，使用了anchor(probe)与negative + positive与negative两种类间距离进行加权表示。

<div align="center" >
<img src="/images/cvpr2017/p2s-8.PNG" width="100%" align="center" />
</div>

并将其修改为P2S的距离形式：

<div align="center" >
<img src="/images/cvpr2017/p2s-4.PNG" width="60%" align="center" />
</div>

<div align="center" >
<img src="/images/cvpr2017/p2s-5.PNG" width="60%" align="center" />
</div>

其中<img src="http://chart.googleapis.com/chart?cht=tx&chl=$P^a_{i,j},N^a_{i,j}$" style="border:none;">表示正样本和负样本指示矩阵：

<div align="center" >
<img src="/images/cvpr2017/p2s-6.PNG" width="40%" align="center" />
</div>

此外在网络结构上，结合了Global Sub-Network、Local Sub-Network和Fusion Sub-Network用于全局+局部的特征表示和学习，并通过全链接层进行融合，最后concat的特征表示通过P2S的loss layer比较相似性进而对网络进行训练：

<div align="center" >
<img src="/images/cvpr2017/p2s-7.PNG" width="100%" align="center" />
</div>

**Results**

|metric|3DPeS|CUHK01(p=100)|PRID2011|Market-1501(SQ)|Market-1501(MQ)|
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
|**CMC top-1**|**71.16**|**77.34**|**70.71**|**70.72**|**85.78**|
|**mAP**|-|-|-|**44.27**|**55.73**|

<span id="jump18"></span>
## 18. Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network [Back](#back)

**Outline**

- 使用了一种多机位一致性学习的方法最优化整个监控网络全局最优的匹配结果。

**Motivation**

现有的一部分Re-ID数据集由大于2个摄像机拍摄而来，然而人们仍然使用以往针对pairwise数据的方法对问题进行优化。然而pairwise的方法在这种监控网络中并不能获得全局最优的匹配结果。这里P1,P2,P3都是同一个行人，然而Re-ID系统如果直接通过cam b和cam c中的外观信息或许并不会认为这是同一行人，这就是pairwise的方法会给监控网络问题带来的不一致性。而一致性，简单来说就是：**如果知道cam a中的i和cam b中的j为同一行人，而cam b中的j和cam c中的k为同一行人，那么cam a中的i和cam c中k也是同一个行人**。

<div align="center" >
<img src="/images/cvpr2017/cadl-1.PNG" width="50%" align="center" />
</div>

本文采用了Consistent-Aware Deep Learning(CADL)的方法用于监控网络中行人重识别的一致性学习。使用网络提取特征后通过计算余弦距离，构建cam a和cam b行人的相似性矩阵<img src="http://chart.googleapis.com/chart?cht=tx&chl=$C^{a,b}$" style="border:none;">。用<img src="http://chart.googleapis.com/chart?cht=tx&chl=$H^{a,b}$" style="border:none;">指示两个摄像机下对应行人是否为同一行人。这样，具有一致性学习特征的全局最优目标定义为最大化C和H的**点点相乘**(element-wise)：

<div align="center" >
<img src="/images/cvpr2017/cadl-2.PNG" width="50%" align="center" />
</div>

这样，一致性约束将通过以下三项进行约束：

- 二元约束：H的取值应该不是0便是1，否则<img src="http://chart.googleapis.com/chart?cht=tx&chl=$J_B \not= 0$" style="border:none;">

<div align="center" >
<img src="/images/cvpr2017/cadl-3.PNG" width="50%" align="center" />
</div>

- 行列约束：应该保证H的每一行和每一列只有一个“1”

<div align="center" >
<img src="/images/cvpr2017/cadl-4.PNG" width="50%" align="center" />
</div>

- Triplet约束：此处的triplet不是指triplet loss，而是用于表示监控网络的一致性：整个网络的一致性可以用3个cam之间的一致性进行等价表示，即<img src="http://chart.googleapis.com/chart?cht=tx&chl=$a_i=b_j, b_j=c_k \Rightarrow a_i=c_k$" style="border:none;">：

<div align="center" >
<img src="/images/cvpr2017/cadl-5.PNG" width="50%" align="center" />
</div>

在训练时，C和H是交替优化的，整个网络结构如下：

<div align="center" >
<img src="/images/cvpr2017/cadl-6.PNG" width="100%" align="center" />
</div>

**Results**

|metric|Market-1501(SQ)|Market-1501(MQ)|RAiD|WARD|
|:-----:|:-----:|:-----:|:-----:|:-----:|
|**CMC top-1**|**73.84**|**80.85**|**97.21**|**99.51**|
|**mAP**|47.11|55.58|-|-|


[Back](#back)
